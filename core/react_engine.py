# FIXME: Unused import 'GeminiClient' - remove if not needed
#!/usr/bin/env python3
"""Unified ReAct Engine Core - The brain of the development agent.

This module implements the ReAct (Reason, Act, Observe) pattern using Google Gemini
for reasoning and a pluggable tool system for actions.

Consolidated Features:
- Base ReAct pattern implementation
- Redis caching and RAG integration (from enhanced_react_engine.py)
- Autonomous capabilities (from autonomous_react_engine.py)
- Configurable feature flags for all advanced functionality
"""

from collections import defaultdict
from dataclasses import dataclass
from dataclasses import field
from datetime import datetime
from enum import Enum
import hashlib
import json
import logging
from pathlib import Path
from typing import Any

from pydantic import BaseModel
from pydantic import Field
from vertexai.generative_models import GenerativeModel

from gterminal.core.session import SessionManager

# Import tools and session management at top level
from gterminal.core.tools.registry import ToolRegistry

# Import performance optimizations
try:
    from fullstack_agent_rust import RustCache
    from fullstack_agent_rust import RustJsonProcessor

    RUST_AVAILABLE = True
except ImportError:
    RUST_AVAILABLE = False
    logging.warning("Rust extensions not available, using Python fallbacks")

# Import unified Gemini client
try:
    from gterminal.core.unified_gemini_client import GeminiClient
    from gterminal.core.unified_gemini_client import get_gemini_client
except ImportError:
    GeminiClient = None
    get_gemini_client = None

# Import enhanced utilities
try:
    from gterminal.core.json_utils import extract_json_from_llm_response
    from gterminal.core.json_utils import validate_analysis_data
    from gterminal.core.json_utils import validate_plan_data
    from gterminal.core.parameter_validator import ParameterValidationError
    from gterminal.core.parameter_validator import validate_tool_parameters

    ENHANCED_UTILS_AVAILABLE = True
except ImportError:
    ENHANCED_UTILS_AVAILABLE = False

# Import Redis and RAG components (optional)
try:
    from gterminal.automation.memory_rag_agent import ProjectMemoryRAG
    from gterminal.utils.database.redis import RedisManager
    from gterminal.utils.database.redis import RedisUtils

    REDIS_RAG_AVAILABLE = True
except ImportError:
    REDIS_RAG_AVAILABLE = False

logger = logging.getLogger(__name__)


class StepType(str, Enum):
    """Types of steps in the ReAct process."""

    REASON = "reason"
    ACT = "act"
    OBSERVE = "observe"
    COMPLETE = "complete"
    AUTONOMOUS_PLAN = "autonomous_plan"
    GOAL_DECOMPOSE = "goal_decompose"


class Step(BaseModel):
    """Represents a single step in the ReAct process."""

    type: StepType
    description: str
    tool_name: str | None = None
    tool_params: dict[str, Any] | None = None
    result: Any | None = None
    timestamp: datetime = Field(default_factory=datetime.now)


class Plan(BaseModel):
    """Execution plan generated by reasoning."""

    goal: str
    steps: list[Step]
    context: dict[str, Any] = {}
    requires_adjustment: bool = False


class ToolResult(BaseModel):
    """Result from tool execution."""

    success: bool
    data: Any
    error: str | None = None
    execution_time: float = 0.0


class Observation(BaseModel):
    """Observation after executing an action."""

    step_successful: bool
    needs_adjustment: bool
    adjustment_reason: str | None = None
    next_action: str | None = None


class ReactResponse(BaseModel):
    """Final response from the ReAct engine."""

    success: bool
    result: Any
    steps_executed: list[Step]
    total_time: float
    session_id: str
    cached: bool = False
    autonomous_execution: bool = False
    patterns_learned: int = 0


# Autonomous-specific types
class AutonomyLevel(str, Enum):
    """Levels of autonomous operation."""

    MANUAL = "manual"  # User confirms each step
    GUIDED = "guided"  # User confirms major steps
    SEMI_AUTO = "semi_auto"  # User confirms critical steps
    FULLY_AUTO = "fully_auto"  # Full autonomous operation


class TaskComplexity(str, Enum):
    """Task complexity levels."""

    SIMPLE = "simple"  # Single step, straightforward
    MODERATE = "moderate"  # 2-5 steps, some logic
    COMPLEX = "complex"  # 5+ steps, branching logic
    EXPERT = "expert"  # Requires deep analysis


@dataclass
class Dependency:
    """Task dependency definition."""

    step_id: str
    dependency_type: str  # 'requires', 'blocks', 'optional'
    condition: str | None = None


@dataclass
class ContextSnapshot:
    """Snapshot of execution context for persistence."""

    timestamp: datetime = field(default_factory=datetime.now)
    request: str = ""
    goal: str = ""
    completed_steps: list[str] = field(default_factory=list)
    pending_steps: list[str] = field(default_factory=list)
    learned_patterns: dict[str, Any] = field(default_factory=dict)
    execution_metadata: dict[str, Any] = field(default_factory=dict)


class EnhancedContext(BaseModel):
    """Enhanced context with RAG and Redis integration."""

    session_context: dict[str, Any]
    project_knowledge: dict[str, Any] = {}
    similar_patterns: list[dict[str, Any]] = []
    cached_responses: list[dict[str, Any]] = []
    redis_available: bool = False
    rag_available: bool = False


class ReactEngineConfig(BaseModel):
    """Configuration for ReactEngine features."""

    # Redis and caching
    enable_redis: bool = False
    redis_ttl: int = 3600
    cache_responses: bool = True

    # RAG integration
    enable_rag: bool = False
    rag_similarity_threshold: float = 0.8

    # Autonomous features
    enable_autonomous: bool = False
    autonomy_level: AutonomyLevel = AutonomyLevel.MANUAL
    max_autonomous_steps: int = 10
    learn_from_execution: bool = True

    # Performance options
    enable_rust_optimizations: bool = True
    parallel_tool_execution: bool = False

    # Streaming and progress
    enable_streaming: bool = False
    progress_update_interval: float = 1.0


class ReactEngine:
    """Unified ReAct engine that orchestrates reasoning and tool execution.

    This consolidated engine supports:
    - Basic ReAct pattern (always available)
    - Redis caching and RAG integration (optional)
    - Autonomous operation with goal decomposition (optional)
    - Learning from execution patterns (optional)
    """

    def __init__(
        self,
        model: GenerativeModel | None = None,
        config: ReactEngineConfig | None = None,
        project_root: Path | None = None,
        profile: str = "business",
    ) -> None:
        """Initialize the unified ReAct engine.

        Args:
            model: Google Gemini model for reasoning (can be None for tool-only mode)
            config: Configuration for optional features
            project_root: Root directory for project operations
            profile: Profile for Gemini client (business/personal)

        """
        self.config = config or ReactEngineConfig()
        self.project_root = project_root or Path("/home/david/agents/my-fullstack-agent")
        self.profile = profile

        # Initialize model - use provided model or create from profile
        if model:
            self.model = model
        elif get_gemini_client:
            client = get_gemini_client(profile=profile)
            self.model = client.model
        else:
            self.model = None

        # Initialize tool registry
        self.tool_registry = ToolRegistry()

        # Initialize session manager
        self.session_manager = SessionManager()

        # Initialize performance optimizations if available
        if RUST_AVAILABLE and self.config.enable_rust_optimizations:
            self.cache = RustCache(capacity=10000, ttl_seconds=self.config.redis_ttl)
            self.json_processor = RustJsonProcessor()
        else:
            self.cache = {}
            self.json_processor = json

        # Initialize Redis components if enabled
        self.redis_client = None
        self.redis_utils = None
        self.redis_available = False

        # Initialize RAG components if enabled
        self.rag_agent = None
        self.rag_available = False

        # Initialize autonomous components if enabled
        self.learned_patterns: dict[str, list[dict[str, Any]]] = defaultdict(list)
        self.execution_history: list[ContextSnapshot] = []
        self.pattern_cache_file = self.project_root / ".autonomous" / "learned_patterns.json"

        if self.config.enable_autonomous:
            self._load_learned_patterns()

        logger.info(f"✅ Unified ReAct Engine initialized with config: {self.config.model_dump()}")

    # FIXME: Function 'initialize_redis' missing return type annotation
    async def initialize_redis(self) -> None:
        """Initialize Redis connection for enhanced caching."""
        if not self.config.enable_redis or not REDIS_RAG_AVAILABLE:
            return

        try:
            self.redis_client = await RedisManager.get_client("react_engine")
            self.redis_utils = RedisUtils(self.redis_client)
            self.redis_available = True
            logger.info("✅ Redis integration enabled for ReAct engine")
        except Exception as e:
            logger.warning(f"Redis not available, using fallback caching: {e}")
            self.redis_available = False

    # FIXME: Function 'initialize_rag' missing return type annotation
    async def initialize_rag(self) -> None:
        """Initialize RAG agent for enhanced context building."""
        if not self.config.enable_rag or not REDIS_RAG_AVAILABLE:
            return

        try:
            self.rag_agent = ProjectMemoryRAG()
            await self.rag_agent.initialize_memory()
            self.rag_available = True
            logger.info("✅ RAG integration enabled for ReAct engine")
        except Exception as e:
            logger.warning(f"RAG not available, using basic context: {e}")
            self.rag_available = False

    # FIXME: Async function 'process_request' missing error handling - add try/except block
    async def process_request(
        self,
        request: str,
        session_id: str | None = None,
        streaming: bool | None = None,
    ) -> ReactResponse:
        """Process a user request through the ReAct loop.

        Args:
            request: Natural language request from user
            session_id: Session ID for maintaining context
            streaming: Whether to stream intermediate results (overrides config)

        Returns:
            ReactResponse with execution results

        """
        # Use config streaming if not explicitly provided
        if streaming is None:
            streaming = self.config.enable_streaming

        # Initialize optional components if needed
        if self.config.enable_redis and not self.redis_available:
            await self.initialize_redis()
        if self.config.enable_rag and not self.rag_available:
            await self.initialize_rag()

        # Route to appropriate processing method
        if self.config.enable_autonomous:
            return await self._process_autonomous_request(request, session_id, streaming)
        if self.config.enable_redis or self.config.enable_rag:
            return await self._process_enhanced_request(request, session_id, streaming)
        return await self._process_basic_request(request, session_id, streaming)

    async def _process_basic_request(
        self,
        request: str,
        session_id: str | None = None,
        streaming: bool = False,
    ) -> ReactResponse:
        """Process request using basic ReAct pattern (original implementation)."""
        start_time = datetime.now()

        # Get or create session
        session = self.session_manager.get_or_create(session_id or "default")
        session.add_interaction("user", request)

        # Check cache for similar requests
        cache_key = f"request:{request[:50]}"
        if self.config.cache_responses and RUST_AVAILABLE and cache_key in self.cache:
            cached_result = self.cache.get(cache_key)
            if cached_result:
                logger.info(f"Cache hit for request: {request[:50]}...")
                cached_result["cached"] = True
                return ReactResponse(**cached_result)

        steps_executed: list[Any] = []

        try:
            # 1. REASON: Generate plan using Gemini
            plan = await self._reason(request, session)
            steps_executed.append(
                Step(type=StepType.REASON, description=f"Generated plan: {plan.goal}")
            )

            # 2. Execute plan steps
            for step in plan.steps:
                # ACT: Execute the tool
                if step.tool_name:
                    result = await self._act(step.tool_name, step.tool_params or {})
                    step.result = result
                    steps_executed.append(step)

                    # OBSERVE: Evaluate the result
                    observation = await self._observe(step, result, plan)

                    # Adjust plan if needed
                    if observation.needs_adjustment:
                        logger.info(f"Adjusting plan: {observation.adjustment_reason}")
                        plan = await self._adjust_plan(plan, observation, session)

                    # Stream intermediate results if requested
                    if streaming and session.websocket:
                        await self._stream_update(session.websocket, step, observation)

            # 3. COMPLETE: Format final response
            final_result = await self._complete(steps_executed, session)

            response = ReactResponse(
                success=True,
                result=final_result,
                steps_executed=steps_executed,
                total_time=(datetime.now() - start_time).total_seconds(),
                session_id=session.id,
            )

            # Cache successful responses
            if self.config.cache_responses and RUST_AVAILABLE:
                self.cache.set(cache_key, response.model_dump())

            # Save session
            await self.session_manager.save(session.id)

            return response

        except Exception as e:
            logger.exception(f"Error in ReAct process: {e}")
            return ReactResponse(
                success=False,
                result={"error": str(e)},
                steps_executed=steps_executed,
                total_time=(datetime.now() - start_time).total_seconds(),
                session_id=session.id,
            )

    # FIXME: Parameter 'session' missing type annotation
    async def _reason(self, request: str, session) -> Plan:
        """Use Gemini to reason about the request and generate a plan."""
        if self.model is None:
            msg = "Model not available. Please set up authentication to use ReAct reasoning."
            raise ValueError(msg)

        # Build context from session history
        context = self._build_context(session)

        # Get available tools
        available_tools = self.tool_registry.get_tool_descriptions()

        prompt = f"""You are a development assistant using the ReAct pattern.

User Request: {request}

Available Tools:
{json.dumps(available_tools, indent=2)}

Context from Previous Interactions:
{json.dumps(context, indent=2)}

Generate a step-by-step plan to accomplish this request. For each step, specify:
1. What action to take
2. Which tool to use (if any)
3. What parameters to pass to the tool

Respond in JSON format:
{{
    "goal": "Clear description of what we're accomplishing",
    "steps": [
        {{
            "type": "act",
            "description": "What this step does",
            "tool_name": "tool_to_use",
            "tool_params": {{"param1": "value1"}}
        }}
    ],
    "context": {{"any": "relevant context"}}
}}
"""

        response = await self.model.generate_content_async(prompt)

        # Use enhanced JSON extraction if available
        try:
            from gterminal.core.json_utils import extract_json_from_llm_response
            from gterminal.core.json_utils import validate_plan_data

            plan_data = extract_json_from_llm_response(response.text, expected_type="dict")

            # Validate the plan data
            if not validate_plan_data(plan_data):
                logger.warning("Plan data validation failed, attempting basic repair")
                if "goal" not in plan_data:
                    plan_data["goal"] = request
                if "steps" not in plan_data:
                    plan_data["steps"] = []

        except ImportError:
            # Fallback to original parsing
            logger.warning("Enhanced JSON utilities not available, using fallback")
            plan_data = (
                self.json_processor.loads(response.text)
                if RUST_AVAILABLE
                else json.loads(response.text)
            )
        except Exception as e:
            logger.exception(f"JSON extraction failed: {e}")
            # Create minimal fallback plan
            plan_data = {
                "goal": request,
                "steps": [
                    {
                        "type": "act",
                        "description": f"Process request: {request}",
                        "tool_name": "text_response",
                        "tool_params": {"message": f"Processing: {request}"},
                    },
                ],
            }

        return Plan(**plan_data)

    async def _act(self, tool_name: str, params: dict[str, Any]) -> ToolResult:
        """Execute a tool with given parameters."""
        try:
            return await self.tool_registry.execute(tool_name, params)
        except Exception as e:
            logger.exception(f"Tool execution failed: {e}")
            return ToolResult(success=False, data=None, error=str(e))

    async def _observe(self, step: Step, result: ToolResult, plan: Plan) -> Observation:
        """Observe the result of an action and determine next steps."""
        if self.model is None:
            # Simple rule-based observation when model is not available
            return Observation(
                step_successful=result.success,
                needs_adjustment=not result.success,
                adjustment_reason="Tool execution failed" if not result.success else None,
                next_action="continue" if result.success else "adjust",
            )

        prompt = f"""Evaluate the result of this action:

Step: {step.description}
Tool: {step.tool_name}
Result Success: {result.success}
Result Data: {json.dumps(result.data) if result.data else "None"}
Error: {result.error or "None"}

Original Goal: {plan.goal}

Should we:
1. Continue with the plan as-is?
2. Adjust the plan based on this result?
3. Retry this step?
4. Complete the task?

Respond in JSON format:
{{
    "step_successful": true/false,
    "needs_adjustment": true/false,
    "adjustment_reason": "reason if adjustment needed",
    "next_action": "continue/adjust/retry/complete"
}}
"""

        response = await self.model.generate_content_async(prompt)
        observation_data = (
            self.json_processor.loads(response.text)
            if RUST_AVAILABLE
            else json.loads(response.text)
        )

        return Observation(**observation_data)

    async def _adjust_plan(self, plan: Plan, observation: Observation, session: Any) -> Plan:
        """Adjust the plan based on observations."""
        if self.model is None:
            # Simple plan adjustment when model is not available
            # Remove failed steps and return remaining plan
            remaining_steps = [s for s in plan.steps if not s.result or s.result.success]
            return Plan(goal=plan.goal, steps=remaining_steps, context=plan.context)

        _ = session  # Unused for now, but kept for future extension
        prompt = f"""The plan needs adjustment:

Original Goal: {plan.goal}
Reason for Adjustment: {observation.adjustment_reason}
Current Context: {json.dumps(plan.context)}

Remaining Steps: {json.dumps([s.model_dump() for s in plan.steps if not s.result])}

Generate an adjusted plan to accomplish the goal given what we've learned.

Respond in the same JSON format as before.
"""

        response = await self.model.generate_content_async(prompt)
        adjusted_plan_data = (
            self.json_processor.loads(response.text)
            if RUST_AVAILABLE
            else json.loads(response.text)
        )

        return Plan(**adjusted_plan_data)

    async def _complete(self, steps: list[Step], session: Any) -> dict[str, Any]:
        """Generate final response based on executed steps."""
        _ = session  # Unused for now, but kept for future extension
        # Collect successful results
        results: list[Any] = []
        for step in steps:
            if step.result and step.result.success:
                results.append({"step": step.description, "result": step.result.data})

        if self.model is None:
            # Simple completion when model is not available
            summary = f"Executed {len(steps)} steps with {len(results)} successful results."
            return {"summary": summary, "detailed_results": results, "steps_count": len(steps)}

        # Generate summary using Gemini
        prompt = f"""Summarize the completed task:

Steps Executed:
{json.dumps([s.description for s in steps], indent=2)}

Results:
{json.dumps(results, indent=2)}

Provide a concise summary of what was accomplished.
"""

        response = await self.model.generate_content_async(prompt)

        return {"summary": response.text, "detailed_results": results, "steps_count": len(steps)}

    # FIXME: Parameter 'session' missing type annotation
    def _build_context(self, session) -> dict[str, Any]:
        """Build context from session history."""
        recent_interactions = session.get_recent_interactions(limit=5)
        return {
            "session_id": session.id,
            "interaction_count": len(session.interactions),
            "recent_interactions": recent_interactions,
            "current_directory": str(self.project_root),
        }

    # FIXME: Parameter 'websocket' missing type annotation
    async def _stream_update(self, websocket, step: Step, observation: Observation) -> None:
        """Stream intermediate updates via WebSocket."""
        update = {
            "type": "step_update",
            "step": step.model_dump(),
            "observation": observation.model_dump(),
            "timestamp": datetime.now().isoformat(),
        }
        await websocket.send_json(update)

    async def _process_enhanced_request(
        self,
        request: str,
        session_id: str | None = None,
        streaming: bool = False,
    ) -> ReactResponse:
        """Process request with Redis caching and RAG integration."""
        start_time = datetime.now()

        # Get or create session
        session = self.session_manager.get_or_create(session_id or "default")
        session.add_interaction("user", request)

        # Generate cache key for the request
        cache_key = self._generate_cache_key(request, session_id)

        # Check Redis cache first
        cached_response = await self._get_cached_response(cache_key)
        if cached_response:
            logger.info(f"Redis cache hit for request: {request[:50]}...")
            cached_response["cached"] = True
            return ReactResponse(**cached_response)

        # Build enhanced context with RAG and Redis
        enhanced_context = await self._build_enhanced_context(request, session)

        steps_executed: list[Any] = []

        try:
            # 1. REASON: Generate plan using enhanced context
            plan = await self._enhanced_reason(request, session, enhanced_context)
            steps_executed.append(
                Step(type=StepType.REASON, description=f"Generated enhanced plan: {plan.goal}")
            )

            # 2. Execute plan steps with enhanced observation
            for step in plan.steps:
                if step.tool_name:
                    # ACT: Execute the tool
                    result = await self._act(step.tool_name, step.tool_params or {})
                    step.result = result
                    steps_executed.append(step)

                    # OBSERVE: Enhanced observation with context awareness
                    observation = await self._enhanced_observe(step, result, plan, enhanced_context)

                    # Adjust plan if needed
                    if observation.needs_adjustment:
                        logger.info(
                            f"Adjusting plan with enhanced context: {observation.adjustment_reason}"
                        )
                        plan = await self._adjust_plan(plan, observation, session)

                    # Stream intermediate results if requested
                    if streaming and session.websocket:
                        await self._stream_update(session.websocket, step, observation)

            # 3. COMPLETE: Enhanced completion with learning
            final_result = await self._enhanced_complete(steps_executed, session, enhanced_context)

            response = ReactResponse(
                success=True,
                result=final_result,
                steps_executed=steps_executed,
                total_time=(datetime.now() - start_time).total_seconds(),
                session_id=session.id,
            )

            # Cache successful responses in Redis
            await self._cache_response(cache_key, response)

            # Update RAG knowledge if available
            if self.rag_available:
                await self._update_rag_knowledge(
                    request, response, steps_executed, enhanced_context
                )

            # Save session
            await self.session_manager.save(session.id)

            return response

        except Exception as e:
            logger.exception(f"Error in enhanced ReAct process: {e}")
            return ReactResponse(
                success=False,
                result={"error": str(e)},
                steps_executed=steps_executed,
                total_time=(datetime.now() - start_time).total_seconds(),
                session_id=session.id,
            )

    async def _process_autonomous_request(
        self,
        request: str,
        session_id: str | None = None,
        streaming: bool = False,
    ) -> ReactResponse:
        """Process request with full autonomous capabilities."""
        start_time = datetime.now()

        # Get or create session
        session = self.session_manager.get_or_create(session_id or "default")
        session.add_interaction("user", request)

        steps_executed: list[Any] = []
        patterns_learned = 0

        try:
            # 1. ANALYZE: Deep analysis of the request
            analysis = await self._analyze_request(request)
            steps_executed.append(
                Step(
                    type=StepType.GOAL_DECOMPOSE,
                    description=f"Analyzed request - Complexity: {analysis.get('complexity', 'unknown')}",
                ),
            )

            # 2. PLAN: Create autonomous plan with goal decomposition
            autonomous_plan = await self._create_autonomous_plan(request, analysis, session)
            steps_executed.append(
                Step(
                    type=StepType.AUTONOMOUS_PLAN,
                    description=f"Created autonomous plan with {len(autonomous_plan.steps)} steps",
                ),
            )

            # 3. EXECUTE: Execute plan with autonomy controls
            for i, step in enumerate(autonomous_plan.steps):
                # Check if user confirmation needed
                if await self._needs_user_confirmation(step, self.config.autonomy_level):
                    logger.info(f"Step {i + 1} requires user confirmation: {step.description}")
                    # In a real implementation, this would pause for user input

                # Execute step with retry logic
                result = await self._execute_step_with_retry(step, max_retries=3)
                step.result = result
                steps_executed.append(step)

                # Assess if plan needs adjustment
                if await self._assess_plan_adjustment(step, result, autonomous_plan):
                    logger.info("Adjusting autonomous plan based on execution results")
                    autonomous_plan.steps[i + 1 :]
                    autonomous_plan = await self._create_autonomous_plan(
                        request,
                        analysis,
                        session,
                        partial_completion=steps_executed,
                    )

                # Stream progress updates
                if streaming:
                    await self._stream_progress_update(
                        session.websocket, step, i + 1, len(autonomous_plan.steps)
                    )

                # Learn from successful patterns
                if result.success and self.config.learn_from_execution:
                    pattern = self._extract_execution_pattern(step, result)
                    if pattern:
                        self.learned_patterns[analysis.get("task_type", "general")].append(pattern)
                        patterns_learned += 1

            # 4. COMPLETE: Finalize with learning
            final_result = await self._complete_autonomous_execution(
                steps_executed, session, patterns_learned
            )

            # Create context snapshot for persistence
            context_snapshot = self._create_context_snapshot(
                request, autonomous_plan, steps_executed, patterns_learned
            )
            await self._persist_context(context_snapshot)

            response = ReactResponse(
                success=True,
                result=final_result,
                steps_executed=steps_executed,
                total_time=(datetime.now() - start_time).total_seconds(),
                session_id=session.id,
                autonomous_execution=True,
                patterns_learned=patterns_learned,
            )

            # Save learned patterns
            if patterns_learned > 0:
                await self._save_learned_patterns()

            # Save session
            await self.session_manager.save(session.id)

            return response

        except Exception as e:
            logger.exception(f"Error in autonomous ReAct process: {e}")
            return ReactResponse(
                success=False,
                result={"error": str(e)},
                steps_executed=steps_executed,
                total_time=(datetime.now() - start_time).total_seconds(),
                session_id=session.id,
                autonomous_execution=True,
            )

    # Enhanced helper methods (from enhanced_react_engine.py)
    def _generate_cache_key(self, request: str, session_id: str | None) -> str:
        """Generate a unique cache key for the request."""
        key_data = f"{request}:{session_id or 'default'}"
        return f"react:response:{hashlib.sha256(key_data.encode()).hexdigest()[:16]}"

    async def _get_cached_response(self, cache_key: str) -> dict[str, Any] | None:
        """Retrieve cached response from Redis."""
        if not self.redis_available:
            return None

        try:
            cached_data = await self.redis_utils.get_json(cache_key)
            if cached_data:
                logger.info(f"Retrieved cached response: {cache_key}")
                return cached_data
        except Exception as e:
            logger.warning(f"Cache retrieval failed: {e}")

        return None

    async def _cache_response(self, cache_key: str, response: ReactResponse) -> None:
        """Cache response in Redis."""
        if not self.redis_available:
            return

        try:
            await self.redis_utils.set_json(
                cache_key, response.model_dump(), expire=self.config.redis_ttl
            )
            logger.info(f"Cached response: {cache_key}")
        except Exception as e:
            logger.warning(f"Cache storage failed: {e}")

    # FIXME: Parameter 'session' missing type annotation
    async def _build_enhanced_context(self, request: str, session) -> EnhancedContext:
        """Build enhanced context with RAG and Redis data."""
        base_context = self._build_context(session)

        enhanced_context = EnhancedContext(
            session_context=base_context,
            redis_available=self.redis_available,
            rag_available=self.rag_available,
        )

        # Query RAG for relevant knowledge
        if self.rag_available:
            enhanced_context.project_knowledge = await self._query_rag_knowledge(request)
            enhanced_context.similar_patterns = await self._find_similar_patterns(request)

        # Get similar cached requests from Redis
        if self.redis_available:
            enhanced_context.cached_responses = await self._get_similar_cached_requests(request)

        return enhanced_context

    async def _query_rag_knowledge(self, request: str) -> dict[str, Any]:
        """Query RAG system for relevant knowledge."""
        if not self.rag_available:
            return {}

        try:
            knowledge = await self.rag_agent.query_knowledge(request)
            return knowledge or {}
        except Exception as e:
            logger.warning(f"RAG query failed: {e}")
            return {}

    async def _find_similar_patterns(self, request: str) -> list[dict[str, Any]]:
        """Find similar execution patterns from history."""
        patterns = []

        # Search in learned patterns (autonomous feature)
        if self.config.enable_autonomous:
            for task_patterns in self.learned_patterns.values():
                for pattern in task_patterns:
                    # Simple similarity check - could be enhanced with embeddings
                    if any(keyword in request.lower() for keyword in pattern.get("keywords", [])):
                        patterns.append(pattern)

        # Search in RAG if available
        if self.rag_available:
            try:
                rag_patterns = await self.rag_agent.find_similar_executions(request)
                patterns.extend(rag_patterns or [])
            except Exception as e:
                logger.warning(f"Pattern search failed: {e}")

        return patterns[:5]  # Limit to top 5 patterns

    async def _get_similar_cached_requests(self, request: str) -> list[dict[str, Any]]:
        """Get similar requests from Redis cache."""
        if not self.redis_available:
            return []

        try:
            # In a real implementation, this would use Redis search or similar
            # For now, return empty list
            return []
        except Exception as e:
            logger.warning(f"Similar request search failed: {e}")
            return []

    # FIXME: Parameter 'session' missing type annotation
    async def _enhanced_reason(
        self, request: str, session, enhanced_context: EnhancedContext
    ) -> Plan:
        """Enhanced reasoning with additional context."""
        if self.model is None:
            msg = "Model not available for enhanced reasoning"
            raise ValueError(msg)

        # Build comprehensive prompt with enhanced context
        prompt = self._build_enhanced_reasoning_prompt(request, session, enhanced_context)

        response = await self.model.generate_content_async(prompt)

        # Use enhanced JSON extraction
        if ENHANCED_UTILS_AVAILABLE:
            plan_data = extract_json_from_llm_response(response.text, expected_type="dict")
            if not validate_plan_data(plan_data):
                plan_data = self._repair_plan_data(plan_data, request)
        else:
            plan_data = json.loads(response.text)

        return Plan(**plan_data)

    # FIXME: Parameter 'step' missing type annotation
    async def _enhanced_observe(
        self, step, result, plan, enhanced_context: EnhancedContext
    ) -> Observation:
        """Enhanced observation with context awareness."""
        if self.model is None:
            return await self._observe(step, result, plan)

        # Include enhanced context in observation
        prompt = self._build_enhanced_observation_prompt(step, result, plan, enhanced_context)

        response = await self.model.generate_content_async(prompt)
        observation_data = json.loads(response.text)

        return Observation(**observation_data)

    # FIXME: Parameter 'steps' missing type annotation
    async def _enhanced_complete(
        self, steps, session, enhanced_context: EnhancedContext
    ) -> dict[str, Any]:
        """Enhanced completion with learning integration."""
        base_result = await self._complete(steps, session)

        # Add enhanced information
        base_result["enhanced_execution"] = True
        base_result["context_used"] = {
            "rag_knowledge": bool(enhanced_context.project_knowledge),
            "similar_patterns": len(enhanced_context.similar_patterns),
            "cached_responses": len(enhanced_context.cached_responses),
        }

        return base_result

    # FIXME: Parameter 'request' missing type annotation
    async def _update_rag_knowledge(self, request, response, steps, enhanced_context) -> None:
        """Update RAG system with execution results."""
        if not self.rag_available:
            return

        try:
            execution_data = {
                "request": request,
                "success": response.success,
                "steps": [s.model_dump() for s in steps],
                "context": enhanced_context.model_dump(),
                "timestamp": datetime.now().isoformat(),
            }

            await self.rag_agent.store_execution(execution_data)
            logger.info("Updated RAG knowledge with execution results")
        except Exception as e:
            logger.warning(f"RAG update failed: {e}")

    # Autonomous helper methods (from autonomous_react_engine.py)
    async def _analyze_request(self, request: str) -> dict[str, Any]:
        """Analyze request to determine complexity and approach."""
        if self.model is None:
            # Simple analysis without model
            return {
                "complexity": TaskComplexity.MODERATE,
                "estimated_steps": 3,
                "task_type": "general",
                "requires_planning": True,
            }

        prompt = f"""Analyze this request and determine:
        1. Task complexity (simple/moderate/complex/expert)
        2. Estimated number of steps
        3. Task type/category
        4. Key requirements and constraints

        Request: {request}

        Respond in JSON format.
        """

        response = await self.model.generate_content_async(prompt)

        if ENHANCED_UTILS_AVAILABLE:
            analysis = extract_json_from_llm_response(response.text, expected_type="dict")
        else:
            analysis = json.loads(response.text)

        return analysis

    # FIXME: Parameter 'request' missing type annotation
    async def _create_autonomous_plan(
        self, request, analysis, session, partial_completion=None
    ) -> Plan:
        """Create autonomous plan with goal decomposition."""
        if self.model is None:
            # Fallback to basic planning
            return await self._reason(request, session)

        # Find similar successful patterns
        similar_patterns = await self._find_similar_patterns(request)

        prompt = self._build_autonomous_planning_prompt(
            request,
            analysis,
            session,
            similar_patterns,
            partial_completion,
        )

        response = await self.model.generate_content_async(prompt)

        if ENHANCED_UTILS_AVAILABLE:
            plan_data = extract_json_from_llm_response(response.text, expected_type="dict")
        else:
            plan_data = json.loads(response.text)

        # Add dependencies and priorities
        plan_data = self._enhance_plan_with_dependencies(plan_data)

        return Plan(**plan_data)

    async def _execute_step_with_retry(self, step: Step, max_retries: int = 3) -> ToolResult:
        """Execute step with retry logic."""
        last_error = None

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    logger.info(f"Retry attempt {attempt + 1} for step: {step.description}")

                result = await self._act(step.tool_name, step.tool_params or {})

                if result.success:
                    return result

                last_error = result.error

                # Don't retry certain errors
                if (
                    "permission" in str(last_error).lower()
                    or "not found" in str(last_error).lower()
                ):
                    break

            except Exception as e:
                last_error = str(e)
                logger.exception(f"Step execution error (attempt {attempt + 1}): {e}")

        return ToolResult(success=False, data=None, error=str(last_error))

    async def _needs_user_confirmation(self, step: Step, autonomy_level: AutonomyLevel) -> bool:
        """Determine if step needs user confirmation based on autonomy level."""
        if autonomy_level == AutonomyLevel.FULLY_AUTO:
            return False
        if autonomy_level == AutonomyLevel.MANUAL:
            return True

        # For guided and semi-auto, check step criticality
        critical_keywords = ["delete", "remove", "drop", "destroy", "production", "deploy"]
        step_text = step.description.lower()

        is_critical = any(keyword in step_text for keyword in critical_keywords)

        if autonomy_level == AutonomyLevel.GUIDED:
            return is_critical
        # SEMI_AUTO
        return is_critical and "production" in step_text

    async def _assess_plan_adjustment(self, step: Step, result: ToolResult, plan: Plan) -> bool:
        """Assess if plan needs adjustment based on execution results."""
        if not result.success:
            return True

        # Check if result significantly differs from expectations
        if step.tool_name == "file_read" and not result.data:
            return True

        # More sophisticated checks could be added here
        return False

    # FIXME: Parameter 'steps' missing type annotation
    async def _complete_autonomous_execution(
        self, steps, session, patterns_learned: int
    ) -> dict[str, Any]:
        """Complete autonomous execution with summary."""
        base_result = await self._complete(steps, session)

        base_result["autonomous_execution"] = True
        base_result["patterns_learned"] = patterns_learned
        base_result["execution_stats"] = {
            "total_steps": len(steps),
            "successful_steps": sum(1 for s in steps if s.result and s.result.success),
            "failed_steps": sum(1 for s in steps if s.result and not s.result.success),
            "retried_steps": sum(
                1 for s in steps if hasattr(s, "retry_count") and s.retry_count > 0
            ),
        }

        return base_result

    # FIXME: Parameter 'request' missing type annotation
    def _create_context_snapshot(self, request, plan, steps, patterns_learned) -> ContextSnapshot:
        """Create snapshot of execution context."""
        return ContextSnapshot(
            request=request,
            goal=plan.goal,
            completed_steps=[s.description for s in steps if s.result and s.result.success],
            pending_steps=[s.description for s in steps if not s.result],
            learned_patterns={"count": patterns_learned},
            execution_metadata={
                "total_steps": len(steps),
                "success_rate": sum(1 for s in steps if s.result and s.result.success)
                / max(len(steps), 1),
            },
        )

    async def _persist_context(self, context: ContextSnapshot) -> None:
        """Persist context snapshot for future sessions."""
        self.execution_history.append(context)

        # Keep only recent history
        if len(self.execution_history) > 100:
            self.execution_history = self.execution_history[-100:]

        # Save to file if autonomous learning is enabled
        if self.config.learn_from_execution:
            try:
                history_file = self.project_root / ".autonomous" / "execution_history.json"
                history_file.parent.mkdir(exist_ok=True)

                history_data = [self._serialize_context(c) for c in self.execution_history]

                # FIXME: Blocking operation 'open' in async function - use async alternative
                with open(history_file, "w") as f:
                    json.dump(history_data, f, indent=2)

            except Exception as e:
                logger.warning(f"Failed to persist context: {e}")

    # FIXME: Parameter 'websocket' missing type annotation
    async def _stream_progress_update(self, websocket, step, current, total) -> None:
        """Stream progress update for autonomous execution."""
        if not websocket:
            return

        update = {
            "type": "autonomous_progress",
            "current_step": current,
            "total_steps": total,
            "step_description": step.description,
            "step_status": "success" if step.result and step.result.success else "in_progress",
            "timestamp": datetime.now().isoformat(),
        }

        await websocket.send_json(update)

    def _extract_execution_pattern(self, step: Step, result: ToolResult) -> dict[str, Any] | None:
        """Extract reusable pattern from successful execution."""
        if not result.success or not step.tool_name:
            return None

        # Extract keywords from step description
        keywords = [word.lower() for word in step.description.split() if len(word) > 3]

        return {
            "tool": step.tool_name,
            "params_template": step.tool_params,
            "keywords": keywords,
            "description_template": step.description,
            "success_indicators": result.data if isinstance(result.data, dict) else {},
            "timestamp": datetime.now().isoformat(),
        }

    def _load_learned_patterns(self) -> None:
        """Load previously learned patterns."""
        if not self.pattern_cache_file.exists():
            return

        try:
            with open(self.pattern_cache_file) as f:
                patterns_data = json.load(f)

            for task_type, patterns in patterns_data.items():
                self.learned_patterns[task_type] = patterns

            logger.info(
                f"Loaded {sum(len(p) for p in self.learned_patterns.values())} learned patterns"
            )
        except Exception as e:
            logger.warning(f"Failed to load learned patterns: {e}")

    async def _save_learned_patterns(self) -> None:
        """Save learned patterns to disk."""
        try:
            self.pattern_cache_file.parent.mkdir(exist_ok=True)

            # FIXME: Blocking operation 'open' in async function - use async alternative
            with open(self.pattern_cache_file, "w") as f:
                json.dump(dict(self.learned_patterns), f, indent=2)

            logger.info(
                f"Saved {sum(len(p) for p in self.learned_patterns.values())} learned patterns"
            )
        except Exception as e:
            logger.warning(f"Failed to save learned patterns: {e}")

    # Helper methods for prompts
    # FIXME: Parameter 'request' missing type annotation
    def _build_enhanced_reasoning_prompt(self, request, session, enhanced_context) -> str:
        """Build comprehensive prompt for enhanced reasoning."""
        base_context = self._build_context(session)

        return f"""You are an enhanced development assistant using the ReAct pattern with additional context.

User Request: {request}

Available Tools:
{json.dumps(self.tool_registry.get_tool_descriptions(), indent=2)}

Base Context:
{json.dumps(base_context, indent=2)}

Project Knowledge (from RAG):
{json.dumps(enhanced_context.project_knowledge, indent=2)}

Similar Successful Patterns:
{json.dumps(enhanced_context.similar_patterns, indent=2)}

Generate an optimized plan based on the available context and patterns.
Respond in JSON format with goal and steps.
"""

    # FIXME: Parameter 'step' missing type annotation
    def _build_enhanced_observation_prompt(self, step, result, plan, enhanced_context) -> str:
        """Build prompt for enhanced observation."""
        return f"""Evaluate this action result with enhanced context:

Step: {step.description}
Result: {json.dumps(result.model_dump())}
Original Goal: {plan.goal}

Similar Pattern Outcomes:
{json.dumps([p.get("success_indicators", {}) for p in enhanced_context.similar_patterns[:3]], indent=2)}

Determine if we should continue, adjust, or complete.
Respond in JSON format.
"""

    # FIXME: Parameter 'request' missing type annotation
    def _build_autonomous_planning_prompt(
        self, request, analysis, session, patterns, partial=None
    ) -> str:
        """Build prompt for autonomous planning."""
        context = self._build_context(session)

        prompt = f"""Create an autonomous execution plan with goal decomposition.

Request: {request}

Analysis:
{json.dumps(analysis, indent=2)}

Context:
{json.dumps(context, indent=2)}

Similar Successful Patterns:
{json.dumps(patterns, indent=2)}
"""

        if partial:
            prompt += f"\n\nPartially Completed Steps:\n{json.dumps([s.description for s in partial], indent=2)}\n"

        prompt += """\nGenerate a detailed plan with:
1. Clear goal decomposition
2. Step dependencies
3. Success criteria
4. Fallback strategies

Respond in JSON format.
"""

        return prompt

    def _enhance_plan_with_dependencies(self, plan_data: dict) -> dict:
        """Add dependencies and priorities to plan."""
        # Simple dependency inference based on step order
        for i, step in enumerate(plan_data.get("steps", [])):
            if i > 0:
                # Each step depends on the previous by default
                step["dependencies"] = [plan_data["steps"][i - 1].get("id", f"step_{i - 1}")]
            step["priority"] = len(plan_data.get("steps", [])) - i

        return plan_data

    def _repair_plan_data(self, plan_data: dict, request: str) -> dict:
        """Repair incomplete plan data."""
        if "goal" not in plan_data:
            plan_data["goal"] = request
        if "steps" not in plan_data:
            plan_data["steps"] = []
        if not plan_data["steps"]:
            plan_data["steps"] = [
                {
                    "type": "act",
                    "description": f"Process request: {request}",
                    "tool_name": "text_response",
                    "tool_params": {"message": request},
                },
            ]

        return plan_data

    def _serialize_context(self, context: ContextSnapshot) -> dict:
        """Serialize context snapshot for storage."""
        return {
            "timestamp": context.timestamp.isoformat(),
            "request": context.request,
            "goal": context.goal,
            "completed_steps": context.completed_steps,
            "pending_steps": context.pending_steps,
            "learned_patterns": context.learned_patterns,
            "execution_metadata": context.execution_metadata,
        }

    # Public utility methods
    # FIXME: Async function 'get_engine_status' missing error handling - add try/except block
    async def get_engine_status(self) -> dict[str, Any]:
        """Get comprehensive engine status."""
        status = {
            "config": self.config.model_dump(),
            "model_available": self.model is not None,
            "redis_available": self.redis_available,
            "rag_available": self.rag_available,
            "rust_optimizations": RUST_AVAILABLE and self.config.enable_rust_optimizations,
            "learned_patterns_count": sum(len(p) for p in self.learned_patterns.values()),
            "execution_history_count": len(self.execution_history),
            "active_sessions": len(self.session_manager.sessions),
        }

        # Add cache stats if available
        if RUST_AVAILABLE and hasattr(self.cache, "stats"):
            status["cache_stats"] = self.cache.stats()

        return status
